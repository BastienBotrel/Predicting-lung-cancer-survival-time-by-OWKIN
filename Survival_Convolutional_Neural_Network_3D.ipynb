{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Survival Convolutional Neural Network_3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CMLB6W9it-K"
      },
      "source": [
        "# **Survival Convolutional Neural Network** (from 3D CT scans)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gLcPNcrlX7_"
      },
      "source": [
        "import tensorflow as tf \r\n",
        "import pandas as pd\r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy \r\n",
        "from typing import Dict, Iterable, Sequence, Tuple, Optional\r\n",
        "import pathlib\r\n",
        "from pathlib import Path\r\n",
        "pip install lifelines\r\n",
        "from lifelines.utils import concordance_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZM7pfC6yVjN"
      },
      "source": [
        "## **Loading Data and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0w5Jbo2QC8y"
      },
      "source": [
        "### **CT scans**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-33FEhMaHF"
      },
      "source": [
        "train_folder = pathlib.Path(\"/content/drive/My Drive/x_train/images\")\n",
        "all_image_paths = [str(img_path) for img_path in list(train_folder.glob(\"*\"))]\n",
        "all_image_paths = sorted(all_image_paths)\n",
        "images=all_image_paths\n",
        "len(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_EmuSHyO70Z"
      },
      "source": [
        "CT scans store raw voxel intensity in Hounsfield units (HU). \r\n",
        "This is defined as Air = −1000 HU, Lung ≈ −500 HU, Water = 0 HU, Soft tissue (& blood) ≈ +50 HU, Bone ≈ +1000 HU.\r\n",
        "A lung window from -1200 to +800 can be applied to view lung tumor and normalize CT scans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJbvIV49MaOi"
      },
      "source": [
        "def read_file(filepath):\n",
        "    # Read file\n",
        "    scan = np.load(filepath)\n",
        "    # Get raw data\n",
        "    scan = scan['scan']\n",
        "    return scan\n",
        "\n",
        "def normalize(volume):\n",
        "    min = -1200\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "def process_scan(path):\n",
        "    # Read scan\n",
        "    volume = read_file(path)\n",
        "    # Normalize\n",
        "    volume = normalize(volume)\n",
        "    return volume"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA9s4zxBNnP0"
      },
      "source": [
        "# Each scan is rescaled.\n",
        "normal_scans = np.array([process_scan(path) for path in images])\n",
        "normal_scans.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4son-NwbP_tt"
      },
      "source": [
        "### **Target and Censorship Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsAZHYPVWX6f"
      },
      "source": [
        "y_train = pd.read_csv( '/content/drive/My Drive/y_train.csv', index_col=0)\n",
        "y_train=y_train.sort_index()\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d1DfukGQi-y"
      },
      "source": [
        "### **Splitting Train Data in Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbErDL94WbWt"
      },
      "source": [
        "survivaltime=[x for x in (y_train.SurvivalTime)]\n",
        "event=[x for x in (y_train.Event)]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcnPGpiCWdcx"
      },
      "source": [
        "# Split data in the ratio 70-30 for training and validation.\n",
        "x_train = normal_scans[40:]\n",
        "time_train = np.array(survivaltime[40:])\n",
        "event_train = np.array(event[40:])\n",
        "\n",
        "x_test = normal_scans[:40]\n",
        "time_test = np.array(survivaltime[:40])\n",
        "event_test = np.array(event[:40])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yljhGcVto3pq"
      },
      "source": [
        "### **Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rBIhW1Vp2Bv"
      },
      "source": [
        "The number of train data being quite small and the Convolutional Neural Network having the tendency to overfit quite quickly, I tried several techniques of data augmentation. These techniques did not improve my results. However, I let the code which could be used in other cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NaYXeSCo-Bt"
      },
      "source": [
        "pip install dltk\r\n",
        "from dltk.io.augmentation import *\r\n",
        "from dltk.io.preprocessing import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O71zt76k-Ml0"
      },
      "source": [
        "## **Creation of Convolution Neural Network for Survival Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y7REcHHl0eL"
      },
      "source": [
        "### **Train and Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoxESiH-9xOV"
      },
      "source": [
        "def _make_riskset(time: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute mask that represents each sample's risk set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed event time sorted in descending order.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    risk_set : np.ndarray, shape=(n_samples, n_samples)\n",
        "        Boolean matrix where the `i`-th row denotes the\n",
        "        risk set of the `i`-th instance, i.e. the indices `j`\n",
        "        for which the observer time `y_j >= y_i`.\n",
        "    \"\"\"\n",
        "    assert time.ndim == 1, \"expected 1D array\"\n",
        "\n",
        "    # sort in descending order\n",
        "    o = np.argsort(-time, kind=\"mergesort\")\n",
        "    n_samples = len(time)\n",
        "    risk_set = np.zeros((n_samples, n_samples), dtype=np.bool_)\n",
        "    for i_org, i_sort in enumerate(o):\n",
        "        ti = time[i_sort]\n",
        "        k = i_org\n",
        "        while k < n_samples and ti == time[o[k]]:\n",
        "            k += 1\n",
        "        risk_set[i_sort, o[:k]] = True\n",
        "    return risk_set\n",
        "\n",
        "\n",
        "def random_rotate3D(img_numpy, min_angle, max_angle):\n",
        "  \"\"\"\n",
        "  3D Medical image rotation\n",
        "  -----\n",
        "  Returns a random rotated array in the same shape\n",
        "  :param img_numpy: 3D numpy array\n",
        "  :param min_angle: in degrees\n",
        "  :param max_angle: in degrees\n",
        "  \"\"\"\n",
        "  liste=[]\n",
        "  for i in range(len(img_numpy)):\n",
        "    assert img_numpy[i].ndim == 3, \"provide a 3d numpy array\"\n",
        "    assert min_angle < max_angle, \"min should be less than max val\"\n",
        "    assert min_angle > -360 or max_angle < 360\n",
        "    all_axes = [(1, 0), (1, 2), (0, 2)]\n",
        "    angle = np.random.randint(low=min_angle, high=max_angle+1)\n",
        "    axes_random_id = np.random.randint(low=0, high=len(all_axes))\n",
        "    axes = all_axes[axes_random_id]\n",
        "    images = scipy.ndimage.rotate(img_numpy[i], angle, reshape=False)\n",
        "    images[images < 0] = 0\n",
        "    images[images > 1] = 1\n",
        "    liste.append(images)\n",
        "    rotated_images=(np.array(liste))\n",
        "  return rotated_images\n",
        "\n",
        "class InputFunction_train:\n",
        "    \"\"\"Callable input function that computes the risk set for each batch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    images : np.ndarray, shape=(n_samples, height, width)\n",
        "        Image data.\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed time.\n",
        "    event : np.ndarray, shape=(n_samples,)\n",
        "        Event indicator.\n",
        "    batch_size : int, optional, default=64\n",
        "        Number of samples per batch.\n",
        "    drop_last : int, optional, default=False\n",
        "        Whether to drop the last incomplete batch.\n",
        "    shuffle : bool, optional, default=False\n",
        "        Whether to shuffle data.\n",
        "    seed : int, optional, default=89\n",
        "        Random number seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 images: np.ndarray,\n",
        "                 time: np.ndarray,\n",
        "                 event: np.ndarray,\n",
        "                 batch_size: int = 20,\n",
        "                 drop_last: bool = False,\n",
        "                 shuffle: bool = False,\n",
        "                 seed: int = 89) -> None:\n",
        "\n",
        "        \"\"\" To be used if we want to apply rotation to the images:\n",
        "        -------\n",
        "        rotated_images = random_rotate3D(images, -20, 20)\n",
        "\n",
        "        if rotated_images.ndim == 4:\n",
        "            images = rotated_images[..., np.newaxis]\n",
        "        \"\"\"\n",
        "\n",
        "        if images.ndim == 4:\n",
        "            images = images[..., np.newaxis]\n",
        "        self.images = images\n",
        "        self.time = time\n",
        "        self.event = event\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"Total number of samples.\"\"\"\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def steps_per_epoch(self) -> int:\n",
        "        \"\"\"Number of batches for one epoch.\"\"\"\n",
        "        return int(np.floor(self.size() / self.batch_size))\n",
        "\n",
        "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
        "\n",
        "        time = self.time[index]\n",
        "        event = self.event[index]\n",
        "        images = self.images[index]\n",
        "\n",
        "        \"\"\" Data augmentation techniques from DLTK library\n",
        "        -------\n",
        "        # Randomly flip the image along axis 1\n",
        "        images = flip(images.copy(), axis=1)\n",
        "        # Add a Gaussian offset \n",
        "        images = add_gaussian_offset(images.copy(), sigma=0.5)\n",
        "        # Add Gaussian noise\n",
        "        images = add_gaussian_noise(images.copy(), sigma=0.15)\n",
        "        \"\"\"\n",
        "\n",
        "        labels = {\n",
        "            \"label_event\": event.astype(np.int32),\n",
        "            \"label_time\": time.astype(np.float32),\n",
        "            \"label_riskset\": _make_riskset(time)\n",
        "        }\n",
        "        return images, labels\n",
        "\n",
        "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
        "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
        "        index = np.arange(self.size())\n",
        "        rnd = np.random.RandomState(self.seed)\n",
        "\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(index)\n",
        "        for b in range(self.steps_per_epoch()):\n",
        "            start = b * self.batch_size\n",
        "            idx = index[start:(start + self.batch_size)]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "        if not self.drop_last:\n",
        "            start = self.steps_per_epoch() * self.batch_size\n",
        "            idx = index[start:]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
        "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
        "        batch_size = self.batch_size if self.drop_last else None\n",
        "        h, w, d, c = self.images.shape[1:]\n",
        "        images = tf.TensorShape([batch_size, h, w, d, c])\n",
        "\n",
        "        labels = {k: tf.TensorShape((batch_size,))\n",
        "                  for k in (\"label_event\", \"label_time\")}\n",
        "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
        "        return images, labels\n",
        "\n",
        "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
        "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
        "        labels = {\"label_event\": tf.int32,\n",
        "                  \"label_time\": tf.float32,\n",
        "                  \"label_riskset\": tf.bool}\n",
        "        return tf.float32, labels\n",
        "\n",
        "    def _make_dataset(self) -> tf.data.Dataset:\n",
        "        \"\"\"Create dataset from generator.\"\"\"\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            self._iter_data,\n",
        "            self._get_dtypes(),\n",
        "            self._get_shapes()\n",
        "        )\n",
        "        return ds\n",
        "\n",
        "    def __call__(self) -> tf.data.Dataset:\n",
        "        return self._make_dataset()\n",
        "\n",
        "class InputFunction_test:\n",
        "    \"\"\"Callable input function that computes the risk set for each batch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    images : np.ndarray, shape=(n_samples, height, width)\n",
        "        Image data.\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed time.\n",
        "    event : np.ndarray, shape=(n_samples,)\n",
        "        Event indicator.\n",
        "    batch_size : int, optional, default=64\n",
        "        Number of samples per batch.\n",
        "    drop_last : int, optional, default=False\n",
        "        Whether to drop the last incomplete batch.\n",
        "    shuffle : bool, optional, default=False\n",
        "        Whether to shuffle data.\n",
        "    seed : int, optional, default=89\n",
        "        Random number seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 images: np.ndarray,\n",
        "                 time: np.ndarray,\n",
        "                 event: np.ndarray,\n",
        "                 batch_size: int = 20,\n",
        "                 drop_last: bool = False,\n",
        "                 shuffle: bool = False,\n",
        "                 seed: int = 89) -> None:\n",
        "        if images.ndim == 4:\n",
        "            images = images[..., np.newaxis]\n",
        "        self.images = images\n",
        "        self.time = time\n",
        "        self.event = event\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"Total number of samples.\"\"\"\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def steps_per_epoch(self) -> int:\n",
        "        \"\"\"Number of batches for one epoch.\"\"\"\n",
        "        return int(np.floor(self.size() / self.batch_size))\n",
        "\n",
        "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
        "        time = self.time[index]\n",
        "        event = self.event[index]\n",
        "        images = self.images[index]\n",
        "\n",
        "        labels = {\n",
        "            \"label_event\": event.astype(np.int32),\n",
        "            \"label_time\": time.astype(np.float32),\n",
        "            \"label_riskset\": _make_riskset(time)\n",
        "        }\n",
        "        return images, labels\n",
        "\n",
        "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
        "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
        "        index = np.arange(self.size())\n",
        "        rnd = np.random.RandomState(self.seed)\n",
        "\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(index)\n",
        "        for b in range(self.steps_per_epoch()):\n",
        "            start = b * self.batch_size\n",
        "            idx = index[start:(start + self.batch_size)]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "        if not self.drop_last:\n",
        "            start = self.steps_per_epoch() * self.batch_size\n",
        "            idx = index[start:]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
        "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
        "        batch_size = self.batch_size if self.drop_last else None\n",
        "        h, w, d, c = self.images.shape[1:]\n",
        "        images = tf.TensorShape([batch_size, h, w, d, c])\n",
        "\n",
        "        labels = {k: tf.TensorShape((batch_size,))\n",
        "                  for k in (\"label_event\", \"label_time\")}\n",
        "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
        "        return images, labels\n",
        "\n",
        "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
        "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
        "        labels = {\"label_event\": tf.int32,\n",
        "                  \"label_time\": tf.float32,\n",
        "                  \"label_riskset\": tf.bool}\n",
        "        return tf.float32, labels\n",
        "\n",
        "    def _make_dataset(self) -> tf.data.Dataset:\n",
        "        \"\"\"Create dataset from generator.\"\"\"\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            self._iter_data,\n",
        "            self._get_dtypes(),\n",
        "            self._get_shapes()\n",
        "        )\n",
        "        return ds\n",
        "\n",
        "    def __call__(self) -> tf.data.Dataset:\n",
        "        return self._make_dataset()\n",
        "\n",
        "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
        "\n",
        "    Note that only risk scores relative to each other matter.\n",
        "    If minimum risk score is negative, we shift scores so minimum\n",
        "    is at zero.\n",
        "    \"\"\"\n",
        "    x_min = tf.reduce_min(x, axis=0)\n",
        "    c = tf.zeros_like(x_min)\n",
        "    norm = tf.where(x_min < 0, -x_min, c)\n",
        "    return x + norm\n",
        "\n",
        "\n",
        "def logsumexp_masked(risk_scores: tf.Tensor,\n",
        "                     mask: tf.Tensor,\n",
        "                     axis: int = 0,\n",
        "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
        "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n",
        "    risk_scores.shape.assert_same_rank(mask.shape)\n",
        "\n",
        "    with tf.name_scope(\"logsumexp_masked\"):\n",
        "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
        "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
        "        # for numerical stability, substract the maximum value\n",
        "        # before taking the exponential\n",
        "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
        "        risk_scores_shift = risk_scores_masked - amax\n",
        "\n",
        "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
        "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
        "        output = amax + tf.math.log(exp_sum)\n",
        "        if not keepdims:\n",
        "            output = tf.squeeze(output, axis=axis)\n",
        "    return output"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k737n4Jpwy98"
      },
      "source": [
        "### **Computation of Cox PH loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSf6kzoVwjJu"
      },
      "source": [
        "class CoxPHLoss(tf.keras.losses.Loss):\r\n",
        "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super().__init__(**kwargs)            \r\n",
        "\r\n",
        "    def call(self,\r\n",
        "             y_true: Sequence[tf.Tensor],\r\n",
        "             y_pred: tf.Tensor) -> tf.Tensor:\r\n",
        "        \"\"\"Compute loss.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        y_true : list|tuple of tf.Tensor\r\n",
        "            The first element holds a binary vector where 1\r\n",
        "            indicates an event 0 censoring.\r\n",
        "            The second element holds the riskset, a\r\n",
        "            boolean matrix where the `i`-th row denotes the\r\n",
        "            risk set of the `i`-th instance, i.e. the indices `j`\r\n",
        "            for which the observer time `y_j >= y_i`.\r\n",
        "            Both must be rank 2 tensors.\r\n",
        "        y_pred : tf.Tensor\r\n",
        "            The predicted outputs. Must be a rank 2 tensor.\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        loss : tf.Tensor\r\n",
        "            Loss for each instance in the batch.\r\n",
        "        \"\"\"\r\n",
        "        event, riskset = y_true\r\n",
        "        predictions = y_pred\r\n",
        "\r\n",
        "        pred_shape = predictions.shape\r\n",
        "        if pred_shape.ndims != 2:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\r\n",
        "                             \"be 2.\" % pred_shape.ndims)\r\n",
        "\r\n",
        "        if pred_shape[1] is None:\r\n",
        "            raise ValueError(\"Last dimension of predictions must be known.\")\r\n",
        "\r\n",
        "        if pred_shape[1] != 1:\r\n",
        "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\r\n",
        "                             \"(received %s) must be 1.\" % pred_shape[1])\r\n",
        "\r\n",
        "        if event.shape.ndims != pred_shape.ndims:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\r\n",
        "                             \"equal rank of event (received %s)\" % (\r\n",
        "                pred_shape.ndims, event.shape.ndims))\r\n",
        "\r\n",
        "        if riskset.shape.ndims != 2:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\r\n",
        "                             \"be 2.\" % riskset.shape.ndims)\r\n",
        "\r\n",
        "        event = tf.cast(event, predictions.dtype)\r\n",
        "        predictions = safe_normalize(predictions)\r\n",
        "\r\n",
        "        with tf.name_scope(\"assertions\"):\r\n",
        "            assertions = (\r\n",
        "                tf.debugging.assert_less_equal(event, 1.),\r\n",
        "                tf.debugging.assert_greater_equal(event, 0.),\r\n",
        "                tf.debugging.assert_type(riskset, tf.bool)\r\n",
        "            )\r\n",
        "\r\n",
        "        # move batch dimension to the end so predictions get broadcast\r\n",
        "        # row-wise when multiplying by riskset\r\n",
        "        pred_t = tf.transpose(predictions)\r\n",
        "        # compute log of sum over risk set for each row\r\n",
        "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\r\n",
        "        assert rr.shape.as_list() == predictions.shape.as_list()\r\n",
        "\r\n",
        "        losses = tf.math.multiply(event, rr - predictions)\r\n",
        "\r\n",
        "        return losses"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxI4wxE78gfe"
      },
      "source": [
        "### **Computation of the Concordance Index** (on the validation data at each epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2nGJha-BiZ"
      },
      "source": [
        "class CindexMetric:\n",
        "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
        "\n",
        "    def reset_states(self) -> None:\n",
        "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
        "        self._data = {\n",
        "            \"label_time\": [],\n",
        "            \"label_event\": [],\n",
        "            \"prediction\": []\n",
        "        }\n",
        "\n",
        "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
        "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : dict\n",
        "            Must have two items:\n",
        "            `label_time`, a tensor containing observed time for one batch,\n",
        "            and `label_event`, a tensor containing event indicator for one batch.\n",
        "        y_pred : tf.Tensor\n",
        "            Tensor containing predicted risk score for one batch.\n",
        "        \"\"\"\n",
        "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
        "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
        "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
        "\n",
        "    def result(self) -> Dict[str, float]:\n",
        "        \"\"\"Computes the concordance index across collected values.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        metrics : dict\n",
        "            Computed metrics.\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        for k, v in self._data.items():\n",
        "            data[k] = np.concatenate(v)\n",
        "\n",
        "        results = concordance_index(\n",
        "            data[\"label_time\"],\n",
        "            data[\"prediction\"],\n",
        "            data[\"label_event\"] == 1,)\n",
        "\n",
        "        result_data = {}\n",
        "        names = \"cindex\"\n",
        "        result_data[names] = 1-results\n",
        "\n",
        "        return result_data"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bSFQQxB4ubg"
      },
      "source": [
        "### **Training of Survival Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6qoSa4n7elO"
      },
      "source": [
        "import tensorflow.compat.v2.summary as summary\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "\n",
        "class TrainAndEvaluateModel:\n",
        "\n",
        "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
        "                 learning_rate, num_epochs):\n",
        "        \"\"\" Note: 'train_dataset' to be removed from __init__ if  \n",
        "        data augmentation is applied \"\"\"\n",
        "\n",
        "        self.num_epochs = num_epochs\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        \"\"\" Note: the line below has to be removed if data augmentation is \n",
        "        applied \"\"\"\n",
        "        self.train_ds = train_dataset\n",
        "\n",
        "        self.val_ds = eval_dataset\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.loss_fn = CoxPHLoss()\n",
        "\n",
        "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
        "        self.val_cindex_metric = CindexMetric()\n",
        "\n",
        "    @tf.function\n",
        "    def train_one_step(self, x, y_event, y_riskset):\n",
        "        y_event = tf.expand_dims(y_event, axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.model(x, training=True)\n",
        "\n",
        "            train_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=logits)\n",
        "\n",
        "        with tf.name_scope(\"gradients\"):\n",
        "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "        return train_loss, logits\n",
        "\n",
        "    def train_and_evaluate(self):\n",
        "        ckpt = tf.train.Checkpoint(\n",
        "            step=tf.Variable(0, dtype=tf.int64),\n",
        "            optimizer=self.optimizer,\n",
        "            model=self.model)\n",
        "        ckpt_manager = tf.train.CheckpointManager(\n",
        "            ckpt, str(self.model_dir), max_to_keep=2)\n",
        "\n",
        "        if ckpt_manager.latest_checkpoint:\n",
        "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
        "\n",
        "        train_summary_writer = summary.create_file_writer(\n",
        "            str(self.model_dir / \"train\"))\n",
        "        val_summary_writer = summary.create_file_writer(\n",
        "            str(self.model_dir / \"valid\"))\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            with train_summary_writer.as_default():\n",
        "                self.train_one_epoch(ckpt.step)\n",
        "\n",
        "            # Run a validation loop at the end of each epoch.\n",
        "            with val_summary_writer.as_default():\n",
        "                self.evaluate(ckpt.step)\n",
        "\n",
        "        save_path = ckpt_manager.save()\n",
        "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
        "\n",
        "    def train_one_epoch(self, step_counter):\n",
        "          \"\"\" Note: the two lines below have to be used if data augmentation\n",
        "              is applied:\n",
        "          \n",
        "              data = InputFunction_train(x_train, time_train, event_train, batch_size=10, shuffle=True, drop_last=True)\n",
        "              for x, y in data():\n",
        "                        \n",
        "              Image preprocessing (can be used for data augmentation)\n",
        "              -------\n",
        "              #Adjust the brightness of images by a random factor.\n",
        "              x = tf.image.random_brightness(x, max_delta=63)\n",
        "              #Adjust the contrast of images by a random factor.\n",
        "              x = tf.image.random_contrast(x, 0.2, 1.8).\n",
        "         \"\"\"\n",
        "\n",
        "          for x, y in self.train_ds:\n",
        "            train_loss, logits = self.train_one_step(\n",
        "                x, y[\"label_event\"], y[\"label_riskset\"])\n",
        "\n",
        "            step = int(step_counter)\n",
        "            if step == 0:\n",
        "                # see https://stackoverflow.com/questions/58843269/display-graph-using-tensorflow-v2-0-in-tensorboard\n",
        "                func = self.train_one_step.get_concrete_function(\n",
        "                    x, y[\"label_event\"], y[\"label_riskset\"])\n",
        "                summary_ops_v2.graph(func.graph, step=0)\n",
        "\n",
        "            # Update training metric.\n",
        "            self.train_loss_metric.update_state(train_loss)\n",
        "\n",
        "            # Log every 200 batches.\n",
        "            if step % 5 == 0:\n",
        "                # Display metrics\n",
        "                mean_loss = self.train_loss_metric.result()\n",
        "                print(f\"step {step}: mean loss = {mean_loss:.4f}\")\n",
        "                # save summaries\n",
        "                summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
        "                # Reset training metrics\n",
        "                self.train_loss_metric.reset_states()\n",
        "\n",
        "            step_counter.assign_add(1)\n",
        "            \n",
        "    @tf.function\n",
        "    def evaluate_one_step(self, x, y_event, y_riskset):\n",
        "        y_event = tf.expand_dims(y_event, axis=1)\n",
        "        val_logits = self.model(x, training=False)\n",
        "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
        "        return val_loss, val_logits\n",
        "\n",
        "    def evaluate(self, step_counter):\n",
        "        self.val_cindex_metric.reset_states()\n",
        "        \n",
        "        for x_val, y_val in self.val_ds:\n",
        "            val_loss, val_logits = self.evaluate_one_step(\n",
        "                x_val, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
        "\n",
        "            # Update val metrics\n",
        "            self.val_loss_metric.update_state(val_loss)\n",
        "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
        "\n",
        "        val_loss = self.val_loss_metric.result()\n",
        "        summary.scalar(\"loss\",\n",
        "                       val_loss,\n",
        "                       step=step_counter)\n",
        "        self.val_loss_metric.reset_states()\n",
        "        \n",
        "        val_cindex = self.val_cindex_metric.result()\n",
        "        for key, value in val_cindex.items():\n",
        "          summary.scalar(key, value, step=step_counter)\n",
        "\n",
        "        print(f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCbNf8vT9sdC"
      },
      "source": [
        "### **Architecture of the Survival Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G818QvE-Ti4L"
      },
      "source": [
        "- The size of the CT scans (grayscale image) is 92x92x92. They are normalized by batch before being fed to the network.\r\n",
        "- Two convolutional layers using a 3x3 kernel size, stride 1, padding='valid', and a ReLu activation function, followed by a batch normalization and a max pooling layer with a pool size of 2, dividing the spatial dimension with a size of 2. \r\n",
        "- Fully connected network, composed of two hidden dense layers and a dense output layer. The final dense layer output the predictions (risk score). A dropout layer with a dropout rate of 10% is added to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f29lpxpDx_rW"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             \n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    tf.keras.layers.Conv3D(filters=16, kernel_size=(3,3,3), activation=\"relu\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling3D(pool_size=(2,2,2)),\n",
        "\n",
        "    tf.keras.layers.Conv3D(filters=32, kernel_size=(3,3,3), activation=\"relu\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling3D(pool_size=(2,2,2)),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(120, activation='relu', name='dense_1'),\n",
        "    tf.keras.layers.Dense(84, activation='relu', name='dense_2'),\n",
        "    tf.keras.layers.Dropout(rate=0.10),\n",
        "    tf.keras.layers.Dense(1, activation='linear', name='dense_3')\n",
        "\n",
        "])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eiphQ_o0a0N"
      },
      "source": [
        "*Note: below some of the changes I tried to improve the results\r\n",
        "- Additional CONV3D and Dense layers \r\n",
        "- Additional filters in the CONV3D layers\r\n",
        "- Batch normalization layers before the activation function\r\n",
        "- Different regularization techniques to reduce the overfitting:\r\n",
        "  - Dropout at higher rates \r\n",
        "  - Dropout layers before every CONV3D layers \r\n",
        "  - L1/L2 regularization\r\n",
        "  - Data augmentation by:\r\n",
        "    - randomly flip the CT scans\r\n",
        "    - adding gaussian noise and offset to the CT scans\r\n",
        "    - randomly rotate the CT scans\r\n",
        "    - randomly adjust the brightness and contrast of the CT scans\r\n",
        "\r\n",
        "  \r\n",
        "None of this provided better results.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5DMUML97sO6"
      },
      "source": [
        "\"\"\" Note: The line below has to be removed if data augmentation is applied. \"\"\"\n",
        "train_fn = InputFunction_train(x_train, time_train, event_train, batch_size=20, shuffle=True, drop_last=True)\n",
        "\n",
        "eval_fn = InputFunction_test(x_test, time_test, event_test)\n",
        "\n",
        "trainer = TrainAndEvaluateModel(\n",
        "    model=model,\n",
        "    #\"\"\" Note: The line below has to be removed if data augmentation is applied. \"\"\"\n",
        "    train_dataset=train_fn(),\n",
        "    model_dir=Path(\"ckpts-mnist-cnn\"),\n",
        "    eval_dataset=eval_fn(),\n",
        "    learning_rate=0.01,\n",
        "    num_epochs=50,\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Z8bbJux_ra"
      },
      "source": [
        "trainer.train_and_evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFjS8IelcQMb"
      },
      "source": [
        "## **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibjDsEHhXmvh"
      },
      "source": [
        "### **Predicted Risk Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqmhwvF740d"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "cph = CoxPHFitter(alpha=0.05)\n",
        "\n",
        "class Predictor:\n",
        "\n",
        "    def __init__(self, model, model_dir):\n",
        "        self.model = model\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        ckpt = tf.train.Checkpoint(\n",
        "            step=tf.Variable(0, dtype=tf.int64),\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            model=self.model)\n",
        "        ckpt_manager = tf.train.CheckpointManager(\n",
        "            ckpt, str(self.model_dir), max_to_keep=2)\n",
        "\n",
        "        if ckpt_manager.latest_checkpoint:\n",
        "            ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
        "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
        "\n",
        "        risk_scores = []\n",
        "        for batch in dataset:\n",
        "            pred = self.model(batch, training=False)\n",
        "            risk_scores.append(pred.numpy())\n",
        "\n",
        "        return np.row_stack(risk_scores)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFCGl_mSezZE"
      },
      "source": [
        "**On Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq5TRLcS76lZ"
      },
      "source": [
        "train_pred_fn = tf.data.Dataset.from_tensor_slices(x_train[..., np.newaxis]).batch(20)\n",
        "\n",
        "predictor = Predictor(model, trainer.model_dir)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnH0Y6vvyVCF"
      },
      "source": [
        "#Predicted risk score of train data\n",
        "train_predictions = predictor.predict(train_pred_fn)\n",
        "\n",
        "risk_score_train=pd.DataFrame(train_predictions)\n",
        "risk_score_train['time_train']=time_train\n",
        "risk_score_train['event_train']=event_train\n",
        "risk_score_train=risk_score_train.rename(columns={0:'risk_score'})\n",
        "risk_score_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKMBEqGte6Xj"
      },
      "source": [
        "**On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxo2y9IRfGge"
      },
      "source": [
        "#Predicted risk score of validation data\r\n",
        "sample_pred_ds = tf.data.Dataset.from_tensor_slices(x_test[..., np.newaxis]).batch(20)\r\n",
        "sample_predictions = predictor.predict(sample_pred_ds)\r\n",
        "\r\n",
        "risk_score_val=pd.DataFrame(sample_predictions)\r\n",
        "risk_score_val['time_test']=time_test\r\n",
        "risk_score_val['event_test']=event_test\r\n",
        "risk_score_val=risk_score_val.rename(columns={0:'risk_score'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3USj8c2Ucwfe"
      },
      "source": [
        "### **Univariate Cox model** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV6Js4T2dDOM"
      },
      "source": [
        "With the predicted risk score as explanatory variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDwMnxj0ynCd"
      },
      "source": [
        "breslow = cph.fit(risk_score_train, duration_col=\"time_train\", event_col=\"event_train\")\n",
        "breslow.print_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGE4s1hhfqoP"
      },
      "source": [
        "### **Multivariate Cox model** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsTVdePFdLGn"
      },
      "source": [
        "With the predicted risk score and the significant clinical variables of the baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1zwaHxFmJvS"
      },
      "source": [
        "**On Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4A3jdkfx_r9"
      },
      "source": [
        "clinical_train=pd.read_csv(\"/content/drive/My Drive/x_train/features/clinical_data_train.csv\", index_col='PatientID')\n",
        "\n",
        "#Preprocessing\n",
        "categories = ['Nstage']\n",
        "clinical_train = pd.get_dummies(clinical_train, columns=categories, drop_first=True)\n",
        "clinical_train=clinical_train.drop(columns=['Histology','Mstage','Tstage'])\n",
        "clinical_train=clinical_train.sort_index()\n",
        "clinical_train=clinical_train.iloc[40:,:]\n",
        "clinical_train=clinical_train.reset_index()\n",
        "clinical_train['predictions']=train_predictions\n",
        "clinical_train['time_train']=risk_score_train.time_train\n",
        "clinical_train['event_train']=risk_score_train.event_train\n",
        "clinical_train.SourceDataset=clinical_train.SourceDataset.apply(lambda x: 0 if x=='l1' else 1)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp_mean = SimpleImputer(strategy='median')\n",
        "clinical_train_= imp_mean.fit_transform(clinical_train)\n",
        "clinical_train=pd.DataFrame(clinical_train_, index=clinical_train.index, columns=clinical_train.columns)\n",
        "clinical_train=clinical_train.drop(columns=['PatientID'])\n",
        "clinical_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMuZ-ohfx_sB"
      },
      "source": [
        "breslow = cph.fit(clinical_train, duration_col=\"time_train\", event_col=\"event_train\")\n",
        "breslow.print_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oin3R_HXl0ph"
      },
      "source": [
        "Note: the Concordance Index is higher when the model is adjusted on Age, Dataset of origin and N-tumoral stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb0llAtYmRil"
      },
      "source": [
        "**On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_viq1lix_sI"
      },
      "source": [
        "clinical_test=pd.read_csv(\"/content/drive/My Drive/x_train/features/clinical_data_train.csv\", index_col='PatientID')\n",
        "\n",
        "#Preprocessing\n",
        "clinical_test['Nstage']=clinical_test.Nstage.apply(lambda x: 3 if x==4 else x)\n",
        "categories = ['Nstage']\n",
        "clinical_test = pd.get_dummies(clinical_test, columns=categories, drop_first=True)\n",
        "clinical_test=clinical_test.drop(columns=['Histology','Mstage', 'Tstage'])\n",
        "clinical_test=clinical_test.sort_index()\n",
        "clinical_test=clinical_test.iloc[:40,:]\n",
        "clinical_test=clinical_test.reset_index()\n",
        "clinical_test['predictions']=sample_predictions\n",
        "clinical_test['time_test']=risk_score_val.time_test\n",
        "clinical_test['event_test']=risk_score_val.event_test\n",
        "clinical_test.SourceDataset=clinical_test.SourceDataset.apply(lambda x: 0 if x=='l1' else 1)\n",
        "\n",
        "clinical_test_= imp_mean.transform(clinical_test)\n",
        "clinical_test=pd.DataFrame(clinical_test_, index=clinical_test.index, columns=clinical_test.columns)\n",
        "clinical_test=clinical_test.drop(columns=['PatientID'])\n",
        "clinical_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaz1CbJwx_sN"
      },
      "source": [
        "# Expected Lifetime\n",
        "clinical_test['predictions']=breslow.predict_expectation(clinical_test)\n",
        "\n",
        "#Concordance Index\n",
        "print(f'Concordance index (lifelines): {concordance_index(clinical_test.time_test, clinical_test.predictions, clinical_test.event_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W48K2lDWoHDO"
      },
      "source": [
        "### **Predictions of Expected Lifetime on Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFP2Kjt04Fd_"
      },
      "source": [
        "# Loading Data\n",
        "test_folder = pathlib.Path(\"/content/drive/My Drive/x_test/images\")\n",
        "all_image_paths_test = [str(img_path) for img_path in list(test_folder.glob(\"*\"))]\n",
        "all_image_paths_test = sorted(all_image_paths_test)\n",
        "normal_scans_test = np.array([process_scan(path) for path in all_image_paths_test])\n",
        "\n",
        "# Predicted Risk Score\n",
        "sample_pred_ds = tf.data.Dataset.from_tensor_slices(normal_scans_test[..., np.newaxis]).batch(20)\n",
        "sample_predictions = predictor.predict(sample_pred_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBRw1LVx_ss"
      },
      "source": [
        "# Preprocessing\n",
        "clinical_test=pd.read_csv(\"/content/drive/My Drive/x_test/features/clinical_data_test.csv\", index_col='PatientID')\n",
        "clinical_test['Nstage']=clinical_test.Nstage.apply(lambda x: 3 if x==4 else x)\n",
        "categories = ['Nstage']\n",
        "clinical_test = pd.get_dummies(clinical_test, columns=categories, drop_first=True)\n",
        "clinical_test=clinical_test.drop(columns=['Histology','Mstage','Tstage'])\n",
        "clinical_test=clinical_test.sort_index()\n",
        "clinical_test=clinical_test.reset_index()\n",
        "clinical_test['predictions']=sample_predictions\n",
        "clinical_test['time_train']=1\n",
        "clinical_test['event_train']=2\n",
        "clinical_test.SourceDataset=clinical_test.SourceDataset.apply(lambda x: 0 if x=='l1' else 1)\n",
        "clinical_test['event_train']\n",
        "\n",
        "clinical_test_= imp_mean.transform(clinical_test)\n",
        "clinical_test=pd.DataFrame(clinical_test_, index=clinical_test.index, columns=clinical_test.columns)\n",
        "clinical_test=clinical_test.drop(columns=['PatientID','time_train','event_train'])\n",
        "clinical_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmcifpF-x_sw"
      },
      "source": [
        "# Expected Lifetime\r\n",
        "pred=breslow.predict_expectation(clinical_test)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxWs51EazGg4"
      },
      "source": [
        "output=pd.DataFrame(data=pred)\n",
        "output=output.rename(columns={0:'SurvivalTime'})\n",
        "output['Event']='nan'\n",
        "output.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}