{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Clinical_SCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UXSreQfGp4s"
      },
      "source": [
        "# **Clinical Survival Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gWqEeSBG5rW"
      },
      "source": [
        "import tensorflow as tf \r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import scipy\r\n",
        "from typing import Dict, Iterable, Sequence, Tuple, Optional\r\n",
        "import pathlib\r\n",
        "from pathlib import Path \r\n",
        "pip install lifelines\r\n",
        "from lifelines.utils import concordance_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRNJDVpa_6ex"
      },
      "source": [
        "## **Loading Data and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urQItPYWHzXY"
      },
      "source": [
        "### **CT scans**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTNGoTt5Oqn3"
      },
      "source": [
        "train_folder = pathlib.Path(\"/content/drive/My Drive/x_train/images\")\n",
        "all_image_paths = [str(img_path) for img_path in list(train_folder.glob(\"*\"))]\n",
        "all_image_paths = sorted(all_image_paths)\n",
        "images=all_image_paths\n",
        "len(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rFkIc_IDTI"
      },
      "source": [
        "CT scans store raw voxel intensity in Hounsfield units (HU). This is defined as Air = −1000 HU, Lung ≈ −500 HU, Water = 0 HU, Soft tissue (& blood) ≈ +50 HU, Bone ≈ +1000 HU. A lung window from -1200 to +800 can be applied to view lung tumor and normalize CT scans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TawIcTCrkeGn"
      },
      "source": [
        "def read_file(filepath):\n",
        "    # Read file\n",
        "    scan = np.load(filepath)\n",
        "    # Get raw data\n",
        "    scan = scan['scan']\n",
        "    return scan\n",
        "\n",
        "def normalize(volume):\n",
        "    min = -1200\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "def process_scan(path):\n",
        "    # Read scan\n",
        "    volume = read_file(path)\n",
        "    # Normalize\n",
        "    volume = normalize(volume)\n",
        "    return volume"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssxdXVYkmUD"
      },
      "source": [
        "# Each scan is rescaled.\n",
        "normal_scans = np.array([process_scan(path) for path in images])\n",
        "normal_scans.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCVLJIDlADyr"
      },
      "source": [
        "### **Clinical Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wf-eE9yAIB8"
      },
      "source": [
        "clinical=pd.read_csv(\"/content/drive/My Drive/x_train/features/clinical_data_train.csv\", index_col='PatientID')\n",
        "\n",
        "#Preprocessing\n",
        "clinical['Tstage']=clinical.Tstage.apply(lambda x: 4 if x==5 else x)\n",
        "clinical['Histology_cat']=clinical.Histology.apply(lambda x: 0 if x in ('Adenocarcinoma','adenocarcinoma') \n",
        "                                                               else 1 if x=='large cell' \n",
        "                                                               else 2 if x in('squamous cell carcinoma', 'Squamous cell carcinoma') \n",
        "                                                               else 3)\n",
        "categories = ['Nstage','Tstage','Histology_cat']\n",
        "clinical = pd.get_dummies(clinical, columns=categories, drop_first=True)\n",
        "clinical=clinical.drop(columns=['Histology','Mstage'])\n",
        "clinical=clinical.sort_index()\n",
        "clinical.SourceDataset=clinical.SourceDataset.apply(lambda x: 0 if x=='l1' else 1)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp_mean = SimpleImputer(strategy='median')\n",
        "clinical_=imp_mean.fit_transform(clinical)\n",
        "clinical=pd.DataFrame(clinical_, index=clinical.index, columns=clinical.columns)\n",
        "clinical.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeurVpVbABaa"
      },
      "source": [
        "### **Target and Censorship Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI1ah0upkr5e"
      },
      "source": [
        "y_train = pd.read_csv('/content/drive/My Drive/y_train.csv', index_col=0)\n",
        "y_train=y_train.sort_index()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXkMITM3Tm7f"
      },
      "source": [
        "### **Splitting Train Data in Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbLLv2KMkxSb"
      },
      "source": [
        "survivaltime=[x for x in (y_train.SurvivalTime)]\n",
        "event=[x for x in (y_train.Event)]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUx-nvcskz5k"
      },
      "source": [
        "# Split data in the ratio 70-30 for training and validation.\n",
        "x_train = normal_scans[:260]\n",
        "time_train = np.array(survivaltime[:260])\n",
        "event_train = np.array(event[:260])\n",
        "clinical_train = clinical.iloc[:260,:].to_numpy(dtype=\"float32\")\n",
        "\n",
        "x_test = normal_scans[260:]\n",
        "time_test = np.array(survivaltime[260:])\n",
        "event_test = np.array(event[260:])\n",
        "clinical_test = clinical.iloc[260:,:].to_numpy(dtype=\"float32\")"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e4IKrRT4Rv"
      },
      "source": [
        "### **Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhTDg34T-dC"
      },
      "source": [
        "The number of train data being quite small and the Clinical Convolutional Neural Network having the tendency to overfit quite quickly, I tried several techniques of data augmentation. These techniques did not improve my results. However, I let the code which could be used in other cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzpfrIuvUCBT"
      },
      "source": [
        "pip install dltk\r\n",
        "from dltk.io.augmentation import *\r\n",
        "from dltk.io.preprocessing import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0C8rnJTUGy9"
      },
      "source": [
        "## **Creation of Clinical Convolution Neural Network for Survival Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yVJq5lMUNei"
      },
      "source": [
        "### **Train and Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O518amxsk7fS"
      },
      "source": [
        "def _make_riskset(time: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute mask that represents each sample's risk set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed event time sorted in descending order.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    risk_set : np.ndarray, shape=(n_samples, n_samples)\n",
        "        Boolean matrix where the `i`-th row denotes the\n",
        "        risk set of the `i`-th instance, i.e. the indices `j`\n",
        "        for which the observer time `y_j >= y_i`.\n",
        "    \"\"\"\n",
        "    assert time.ndim == 1, \"expected 1D array\"\n",
        "\n",
        "    # sort in descending order\n",
        "    o = np.argsort(-time, kind=\"mergesort\")\n",
        "    n_samples = len(time)\n",
        "    risk_set = np.zeros((n_samples, n_samples), dtype=np.bool_)\n",
        "    for i_org, i_sort in enumerate(o):\n",
        "        ti = time[i_sort]\n",
        "        k = i_org\n",
        "        while k < n_samples and ti == time[o[k]]:\n",
        "            k += 1\n",
        "        risk_set[i_sort, o[:k]] = True\n",
        "    return risk_set\n",
        "\n",
        "\n",
        "def random_rotate3D(img_numpy, min_angle, max_angle):\n",
        "  \"\"\"\n",
        "  3D Medical image rotation\n",
        "  -----\n",
        "  Returns a random rotated array in the same shape\n",
        "  :param img_numpy: 3D numpy array\n",
        "  :param min_angle: in degrees\n",
        "  :param max_angle: in degrees\n",
        "  \"\"\"\n",
        "  liste=[]\n",
        "  for i in range(len(img_numpy)):\n",
        "    assert img_numpy[i].ndim == 3, \"provide a 3d numpy array\"\n",
        "    assert min_angle < max_angle, \"min should be less than max val\"\n",
        "    assert min_angle > -360 or max_angle < 360\n",
        "    all_axes = [(1, 0), (1, 2), (0, 2)]\n",
        "    angle = np.random.randint(low=min_angle, high=max_angle+1)\n",
        "    axes_random_id = np.random.randint(low=0, high=len(all_axes))\n",
        "    axes = all_axes[axes_random_id]\n",
        "    images = scipy.ndimage.rotate(img_numpy[i], angle, reshape=False)\n",
        "    images[images < 0] = 0\n",
        "    images[images > 1] = 1\n",
        "    liste.append(images)\n",
        "    rotated_images=(np.array(liste))\n",
        "  return rotated_images\n",
        "\n",
        "class InputFunction_train:\n",
        "    \"\"\"Callable input function that computes the risk set for each batch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    images : np.ndarray, shape=(n_samples, height, width)\n",
        "        Image data.\n",
        "    clinical: np.ndarray, shape=(n_samples, n_variables)\n",
        "      Clinical data.\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed time.\n",
        "    event : np.ndarray, shape=(n_samples,)\n",
        "        Event indicator.\n",
        "    batch_size : int, optional, default=64\n",
        "        Number of samples per batch.\n",
        "    drop_last : int, optional, default=False\n",
        "        Whether to drop the last incomplete batch.\n",
        "    shuffle : bool, optional, default=False\n",
        "        Whether to shuffle data.\n",
        "    seed : int, optional, default=89\n",
        "        Random number seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 images: np.ndarray,\n",
        "                 clinical: np.ndarray,\n",
        "                 time: np.ndarray,\n",
        "                 event: np.ndarray,\n",
        "                 batch_size: int = 20,\n",
        "                 drop_last: bool = False,\n",
        "                 shuffle: bool = False,\n",
        "                 seed: int = 89) -> None:\n",
        "\n",
        "        \"\"\" To be used if we want to apply rotation to the images:\n",
        "        -------\n",
        "        rotated_images = random_rotate3D(images, -20, 20)\n",
        "\n",
        "        if rotated_images.ndim == 4:\n",
        "            images = rotated_images[..., np.newaxis]\n",
        "        \"\"\"\n",
        "\n",
        "        if images.ndim == 4:\n",
        "            images = images[..., np.newaxis]\n",
        "        self.images = images\n",
        "        self.clinical = clinical\n",
        "        self.time = time\n",
        "        self.event = event\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"Total number of samples.\"\"\"\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def steps_per_epoch(self) -> int:\n",
        "        \"\"\"Number of batches for one epoch.\"\"\"\n",
        "        return int(np.floor(self.size() / self.batch_size))\n",
        "\n",
        "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
        "\n",
        "        time = self.time[index]\n",
        "        event = self.event[index]\n",
        "        images = self.images[index]\n",
        "        clinical = self.clinical[index]\n",
        "\n",
        "        \"\"\" Data augmentation techniques from DLTK library\n",
        "        -------\n",
        "        # Randomly flip the image along axis 1\n",
        "        images = flip(images.copy(), axis=1)\n",
        "        # Add a Gaussian offset \n",
        "        images = add_gaussian_offset(images.copy(), sigma=0.5)\n",
        "        # Add Gaussian noise\n",
        "        images = add_gaussian_noise(images.copy(), sigma=0.15)\n",
        "        \"\"\"\n",
        "\n",
        "        labels = {\n",
        "            \"label_event\": event.astype(np.int32),\n",
        "            \"label_time\": time.astype(np.float32),\n",
        "            \"label_riskset\": _make_riskset(time)\n",
        "        }\n",
        "        return images, clinical, labels\n",
        "\n",
        "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
        "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
        "        index = np.arange(self.size())\n",
        "        rnd = np.random.RandomState(self.seed)\n",
        "\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(index)\n",
        "        for b in range(self.steps_per_epoch()):\n",
        "            start = b * self.batch_size\n",
        "            idx = index[start:(start + self.batch_size)]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "        if not self.drop_last:\n",
        "            start = self.steps_per_epoch() * self.batch_size\n",
        "            idx = index[start:]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
        "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
        "        batch_size = self.batch_size if self.drop_last else None\n",
        "        h, w, d, c = self.images.shape[1:]\n",
        "        images = tf.TensorShape([batch_size, h, w, d, c])\n",
        "        clinical = tf.TensorShape([batch_size,self.clinical.shape[1]])\n",
        "\n",
        "        labels = {k: tf.TensorShape((batch_size,))\n",
        "                  for k in (\"label_event\", \"label_time\")}\n",
        "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
        "        return images, clinical, labels\n",
        "\n",
        "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
        "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
        "        labels = {\"label_event\": tf.int32,\n",
        "                  \"label_time\": tf.float32,\n",
        "                  \"label_riskset\": tf.bool}\n",
        "        return tf.float32, tf.float32, labels\n",
        "\n",
        "    def _make_dataset(self) -> tf.data.Dataset:\n",
        "        \"\"\"Create dataset from generator.\"\"\"\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            self._iter_data,\n",
        "            self._get_dtypes(),\n",
        "            self._get_shapes()\n",
        "        )\n",
        "        return ds\n",
        "\n",
        "    def __call__(self) -> tf.data.Dataset:\n",
        "        return self._make_dataset()\n",
        "\n",
        "class InputFunction_test:\n",
        "    \"\"\"Callable input function that computes the risk set for each batch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    images : np.ndarray, shape=(n_samples, height, width)\n",
        "        Image data.\n",
        "    clinical : np.ndarray, shape=(n_samples, n_variables)\n",
        "      Clinical data.\n",
        "    time : np.ndarray, shape=(n_samples,)\n",
        "        Observed time.\n",
        "    event : np.ndarray, shape=(n_samples,)\n",
        "        Event indicator.\n",
        "    batch_size : int, optional, default=64\n",
        "        Number of samples per batch.\n",
        "    drop_last : int, optional, default=False\n",
        "        Whether to drop the last incomplete batch.\n",
        "    shuffle : bool, optional, default=False\n",
        "        Whether to shuffle data.\n",
        "    seed : int, optional, default=89\n",
        "        Random number seed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 images: np.ndarray,\n",
        "                 clinical: np.ndarray,\n",
        "                 time: np.ndarray,\n",
        "                 event: np.ndarray,\n",
        "                 batch_size: int = 20,\n",
        "                 drop_last: bool = False,\n",
        "                 shuffle: bool = False,\n",
        "                 seed: int = 89) -> None:\n",
        "        if images.ndim == 4:\n",
        "            images = images[..., np.newaxis]\n",
        "        self.images = images\n",
        "        self.clinical = clinical\n",
        "        self.time = time\n",
        "        self.event = event\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"Total number of samples.\"\"\"\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def steps_per_epoch(self) -> int:\n",
        "        \"\"\"Number of batches for one epoch.\"\"\"\n",
        "        return int(np.floor(self.size() / self.batch_size))\n",
        "\n",
        "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
        "        time = self.time[index]\n",
        "        event = self.event[index]\n",
        "        images = self.images[index]\n",
        "        clinical = self.clinical[index]\n",
        "\n",
        "        labels = {\n",
        "            \"label_event\": event.astype(np.int32),\n",
        "            \"label_time\": time.astype(np.float32),\n",
        "            \"label_riskset\": _make_riskset(time)\n",
        "        }\n",
        "        return images, clinical, labels\n",
        "\n",
        "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
        "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
        "        index = np.arange(self.size())\n",
        "        rnd = np.random.RandomState(self.seed)\n",
        "\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(index)\n",
        "        for b in range(self.steps_per_epoch()):\n",
        "            start = b * self.batch_size\n",
        "            idx = index[start:(start + self.batch_size)]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "        if not self.drop_last:\n",
        "            start = self.steps_per_epoch() * self.batch_size\n",
        "            idx = index[start:]\n",
        "            yield self._get_data_batch(idx)\n",
        "\n",
        "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
        "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
        "        batch_size = self.batch_size if self.drop_last else None\n",
        "        h, w, d, c = self.images.shape[1:]\n",
        "        images = tf.TensorShape([batch_size, h, w, d, c])\n",
        "        clinical = tf.TensorShape([batch_size,self.clinical.shape[1]])\n",
        "\n",
        "        labels = {k: tf.TensorShape((batch_size,))\n",
        "                  for k in (\"label_event\", \"label_time\")}\n",
        "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
        "        return images, clinical, labels\n",
        "\n",
        "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
        "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
        "        labels = {\"label_event\": tf.int32,\n",
        "                  \"label_time\": tf.float32,\n",
        "                  \"label_riskset\": tf.bool}\n",
        "        return tf.float32, tf.float32, labels\n",
        "\n",
        "    def _make_dataset(self) -> tf.data.Dataset:\n",
        "        \"\"\"Create dataset from generator.\"\"\"\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            self._iter_data,\n",
        "            self._get_dtypes(),\n",
        "            self._get_shapes()\n",
        "        )\n",
        "        return ds\n",
        "\n",
        "    def __call__(self) -> tf.data.Dataset:\n",
        "        return self._make_dataset()\n",
        "\n",
        "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
        "\n",
        "    Note that only risk scores relative to each other matter.\n",
        "    If minimum risk score is negative, we shift scores so minimum\n",
        "    is at zero.\n",
        "    \"\"\"\n",
        "    x_min = tf.reduce_min(x, axis=0)\n",
        "    c = tf.zeros_like(x_min)\n",
        "    norm = tf.where(x_min < 0, -x_min, c)\n",
        "    return x + norm\n",
        "\n",
        "\n",
        "def logsumexp_masked(risk_scores: tf.Tensor,\n",
        "                     mask: tf.Tensor,\n",
        "                     axis: int = 0,\n",
        "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
        "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n",
        "    risk_scores.shape.assert_same_rank(mask.shape)\n",
        "\n",
        "    with tf.name_scope(\"logsumexp_masked\"):\n",
        "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
        "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
        "        # for numerical stability, substract the maximum value\n",
        "        # before taking the exponential\n",
        "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
        "        risk_scores_shift = risk_scores_masked - amax\n",
        "\n",
        "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
        "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
        "        output = amax + tf.math.log(exp_sum)\n",
        "        if not keepdims:\n",
        "            output = tf.squeeze(output, axis=axis)\n",
        "    return output"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBeiIUD4a3zg"
      },
      "source": [
        "### **Computation of Cox PH loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg2cpb-gbESb"
      },
      "source": [
        "class CoxPHLoss(tf.keras.losses.Loss):\r\n",
        "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super().__init__(**kwargs)            \r\n",
        "\r\n",
        "    def call(self,\r\n",
        "             y_true: Sequence[tf.Tensor],\r\n",
        "             y_pred: tf.Tensor) -> tf.Tensor:\r\n",
        "        \"\"\"Compute loss.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        y_true : list|tuple of tf.Tensor\r\n",
        "            The first element holds a binary vector where 1\r\n",
        "            indicates an event 0 censoring.\r\n",
        "            The second element holds the riskset, a\r\n",
        "            boolean matrix where the `i`-th row denotes the\r\n",
        "            risk set of the `i`-th instance, i.e. the indices `j`\r\n",
        "            for which the observer time `y_j >= y_i`.\r\n",
        "            Both must be rank 2 tensors.\r\n",
        "        y_pred : tf.Tensor\r\n",
        "            The predicted outputs. Must be a rank 2 tensor.\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        loss : tf.Tensor\r\n",
        "            Loss for each instance in the batch.\r\n",
        "        \"\"\"\r\n",
        "        event, riskset = y_true\r\n",
        "        predictions = y_pred\r\n",
        "\r\n",
        "        pred_shape = predictions.shape\r\n",
        "        if pred_shape.ndims != 2:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\r\n",
        "                             \"be 2.\" % pred_shape.ndims)\r\n",
        "\r\n",
        "        if pred_shape[1] is None:\r\n",
        "            raise ValueError(\"Last dimension of predictions must be known.\")\r\n",
        "\r\n",
        "        if pred_shape[1] != 1:\r\n",
        "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\r\n",
        "                             \"(received %s) must be 1.\" % pred_shape[1])\r\n",
        "\r\n",
        "        if event.shape.ndims != pred_shape.ndims:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\r\n",
        "                             \"equal rank of event (received %s)\" % (\r\n",
        "                pred_shape.ndims, event.shape.ndims))\r\n",
        "\r\n",
        "        if riskset.shape.ndims != 2:\r\n",
        "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\r\n",
        "                             \"be 2.\" % riskset.shape.ndims)\r\n",
        "\r\n",
        "        event = tf.cast(event, predictions.dtype)\r\n",
        "        predictions = safe_normalize(predictions)\r\n",
        "\r\n",
        "        with tf.name_scope(\"assertions\"):\r\n",
        "            assertions = (\r\n",
        "                tf.debugging.assert_less_equal(event, 1.),\r\n",
        "                tf.debugging.assert_greater_equal(event, 0.),\r\n",
        "                tf.debugging.assert_type(riskset, tf.bool)\r\n",
        "            )\r\n",
        "\r\n",
        "        # move batch dimension to the end so predictions get broadcast\r\n",
        "        # row-wise when multiplying by riskset\r\n",
        "        pred_t = tf.transpose(predictions)\r\n",
        "        # compute log of sum over risk set for each row\r\n",
        "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\r\n",
        "        assert rr.shape.as_list() == predictions.shape.as_list()\r\n",
        "\r\n",
        "        losses = tf.math.multiply(event, rr - predictions)\r\n",
        "\r\n",
        "        return losses"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHEcUZUxbJ-3"
      },
      "source": [
        "### **Computation of the Concordance Index** (on the validation data at each epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AFsSlWFk_G7"
      },
      "source": [
        "class CindexMetric:\n",
        "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
        "\n",
        "    def reset_states(self) -> None:\n",
        "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
        "        self._data = {\n",
        "            \"label_time\": [],\n",
        "            \"label_event\": [],\n",
        "            \"prediction\": []\n",
        "        }\n",
        "\n",
        "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
        "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : dict\n",
        "            Must have two items:\n",
        "            `label_time`, a tensor containing observed time for one batch,\n",
        "            and `label_event`, a tensor containing event indicator for one batch.\n",
        "        y_pred : tf.Tensor\n",
        "            Tensor containing predicted risk score for one batch.\n",
        "        \"\"\"\n",
        "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
        "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
        "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
        "\n",
        "    def result(self) -> Dict[str, float]:\n",
        "        \"\"\"Computes the concordance index across collected values.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        metrics : dict\n",
        "            Computed metrics.\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        for k, v in self._data.items():\n",
        "            data[k] = np.concatenate(v)\n",
        "\n",
        "        results = concordance_index(\n",
        "            data[\"label_time\"],\n",
        "            data[\"prediction\"],\n",
        "            data[\"label_event\"] == 1,)\n",
        "\n",
        "        result_data = {}\n",
        "        names = \"cindex\"\n",
        "        result_data[names] = 1-results\n",
        "\n",
        "        return result_data"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZcm2TBvbQZD"
      },
      "source": [
        "### **Training of Survival Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCN4DAKBlBrN"
      },
      "source": [
        "import tensorflow.compat.v2.summary as summary\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "\n",
        "class TrainAndEvaluateModel:\n",
        "\n",
        "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
        "                 learning_rate, num_epochs):\n",
        "        \"\"\" Note: 'train_dataset' to be removed from __init__ if  \n",
        "        data augmentation is applied \"\"\"\n",
        "\n",
        "        self.num_epochs = num_epochs\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        \"\"\" Note: the line below has to be removed if data augmentation is \n",
        "        applied \"\"\"\n",
        "        self.train_ds = train_dataset\n",
        "\n",
        "        self.val_ds = eval_dataset\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.loss_fn = CoxPHLoss()\n",
        "\n",
        "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
        "        self.val_cindex_metric = CindexMetric()\n",
        "\n",
        "    @tf.function\n",
        "    def train_one_step(self, x_image, x_clinical, y_event, y_riskset):\n",
        "        y_event = tf.expand_dims(y_event, axis=1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.model([x_image, x_clinical], training=True)\n",
        "\n",
        "            train_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=logits)\n",
        "\n",
        "        with tf.name_scope(\"gradients\"):\n",
        "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "        return train_loss, logits\n",
        "\n",
        "    def train_and_evaluate(self):\n",
        "        ckpt = tf.train.Checkpoint(\n",
        "            step=tf.Variable(0, dtype=tf.int64),\n",
        "            optimizer=self.optimizer,\n",
        "            model=self.model)\n",
        "        ckpt_manager = tf.train.CheckpointManager(\n",
        "            ckpt, str(self.model_dir), max_to_keep=2)\n",
        "\n",
        "        if ckpt_manager.latest_checkpoint:\n",
        "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
        "\n",
        "        train_summary_writer = summary.create_file_writer(\n",
        "            str(self.model_dir / \"train\"))\n",
        "        val_summary_writer = summary.create_file_writer(\n",
        "            str(self.model_dir / \"valid\"))\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            with train_summary_writer.as_default():\n",
        "                self.train_one_epoch(ckpt.step)\n",
        "\n",
        "            # Run a validation loop at the end of each epoch.\n",
        "            with val_summary_writer.as_default():\n",
        "                self.evaluate(ckpt.step)\n",
        "\n",
        "        save_path = ckpt_manager.save()\n",
        "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
        "\n",
        "    def train_one_epoch(self, step_counter):\n",
        "\n",
        "          \"\"\" Note: the two lines below have to be used if data augmentation\n",
        "              is applied:\n",
        "          \n",
        "              data = InputFunction_train(x_train, clinical_train, time_train, event_train, batch_size=10, shuffle=True, drop_last=True)\n",
        "              for x_images, x_clinical, y in data():\n",
        "                        \n",
        "              Image preprocessing (can be used for data augmentation)\n",
        "              -------\n",
        "              #Adjust the brightness of images by a random factor.\n",
        "              x_images = tf.image.random_brightness(x_images, 0.2)\n",
        "              #Adjust the contrast of images by a random factor.\n",
        "              x_images = tf.image.random_contrast(x_images, 0.2, 0.5)\n",
        "         \"\"\"\n",
        "\n",
        "          for x_images, x_clinical, y in self.train_ds:\n",
        "            train_loss, logits = self.train_one_step(\n",
        "                x_images, x_clinical, y[\"label_event\"], y[\"label_riskset\"])\n",
        "\n",
        "            step = int(step_counter)\n",
        "            \n",
        "            # Update training metric.\n",
        "            self.train_loss_metric.update_state(train_loss)\n",
        "\n",
        "            # Log every 200 batches.\n",
        "            if step % 5 == 0:\n",
        "                # Display metrics\n",
        "                mean_loss = self.train_loss_metric.result()\n",
        "                print(f\"step {step}: mean loss = {mean_loss:.4f}\")\n",
        "                # save summaries\n",
        "                summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
        "                # Reset training metrics\n",
        "                self.train_loss_metric.reset_states()\n",
        "\n",
        "            step_counter.assign_add(1)\n",
        "\n",
        "    @tf.function\n",
        "    def evaluate_one_step(self, x, x_clinical, y_event, y_riskset):\n",
        "        y_event = tf.expand_dims(y_event, axis=1)\n",
        "        val_logits = self.model([x, x_clinical], training=False)\n",
        "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
        "        return val_loss, val_logits\n",
        "\n",
        "    def evaluate(self, step_counter):\n",
        "        self.val_cindex_metric.reset_states()\n",
        "        \n",
        "        for val_images, val_clinical, y_val in self.val_ds:\n",
        "            val_loss, val_logits = self.evaluate_one_step(\n",
        "                val_images, val_clinical, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
        "\n",
        "            # Update val metrics\n",
        "            self.val_loss_metric.update_state(val_loss)\n",
        "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
        "\n",
        "        val_loss = self.val_loss_metric.result()\n",
        "        summary.scalar(\"loss\",\n",
        "                       val_loss,\n",
        "                       step=step_counter)\n",
        "        self.val_loss_metric.reset_states()\n",
        "        \n",
        "        val_cindex = self.val_cindex_metric.result()\n",
        "        for key, value in val_cindex.items():\n",
        "          summary.scalar(key, value, step=step_counter)\n",
        "\n",
        "        print(f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfFYo2D09ax7"
      },
      "source": [
        "### **Architecture of the Clinical Survival Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moNhSLlt9h75"
      },
      "source": [
        "This neural network handle two inputs: **3D CT scans** and **Clinical data**.\r\n",
        "\r\n",
        "- The 3D CT scans (grayscale image, shape is 92x92x92) are normalized by batch before being fed to the network.\r\n",
        "- Two convolutional layers using a 3x3x3 kernel size, stride 1, padding='valid', and a ReLu activation function, followed by a batch normalization and a max pooling layer with a pool size of 2, dividing the spatial dimension with a size of 2.\r\n",
        "- The output of the second max pooling layer is flattened and concatenated with the clinical data.\r\n",
        "- Fully connected network, composed of two hidden dense layers and a dense output layer. A batch normalization is applied on the output of the two hidden dense layers. The final dense layer output the predictions (risk score). A dropout layer with a dropout rate of 10% is added to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ__7UfIRskH"
      },
      "source": [
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv3D, BatchNormalization, MaxPooling3D, Concatenate, Dropout\r\n",
        "\r\n",
        "image_input = keras.Input(shape=(92,92,92,1), name=\"image_input\")\r\n",
        "x = layers.BatchNormalization()(image_input)\r\n",
        "x = layers.Conv3D(filters=16, kernel_size=(3,3,3), activation=\"relu\", use_bias=False)(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.MaxPooling3D(pool_size=(2,2,2))(x)\r\n",
        "x = layers.Conv3D(filters=32, kernel_size=(3,3,3), activation=\"relu\", use_bias=False)(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.MaxPooling3D(pool_size=(2,2,2))(x)\r\n",
        "block_1_output = layers.Flatten()(x)\r\n",
        "\r\n",
        "numeric_input = keras.Input(shape=(11), name=\"numeric_input\")\r\n",
        "x = Concatenate()([numeric_input, block_1_output])\r\n",
        "x = layers.Dense(120, activation='relu', name='dense_1', use_bias=False)(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.Dense(84, activation='relu', name='dense_2', use_bias=False)(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.Dropout(rate=0.10)(x)\r\n",
        "output = layers.Dense(1, activation='linear', name='dense_3')(x)\r\n",
        "\r\n",
        "model = keras.Model(inputs=[image_input, numeric_input], outputs=output)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXfIUmlEe-QO"
      },
      "source": [
        "\"\"\" Note: The line below has to be removed if data augmentation is applied. \"\"\"\r\n",
        "train_fn = InputFunction_train(x_train, clinical_train, time_train, event_train, batch_size=20, shuffle=True, drop_last=True)\r\n",
        "\r\n",
        "eval_fn = InputFunction_test(x_test, clinical_test, time_test, event_test)\r\n",
        "\r\n",
        "trainer = TrainAndEvaluateModel(\r\n",
        "    model=model,\r\n",
        "    model_dir=Path(\"ckpts-mnist-cnn\"),\r\n",
        "    # Note: The line below has to be removed if data augmentation is applied. \r\n",
        "    train_dataset=train_fn(),\r\n",
        "    eval_dataset=eval_fn(),\r\n",
        "    learning_rate=0.0001,\r\n",
        "    num_epochs=5,\r\n",
        ")"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo1lo8RSfJa1"
      },
      "source": [
        "trainer.train_and_evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mbtrzccPxj6"
      },
      "source": [
        "## **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uon9S9erP4JV"
      },
      "source": [
        "### **Predicted Risk Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2wImlSSJpAx"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "cph = CoxPHFitter(alpha=0.05)\n",
        "\n",
        "class Predictor:\n",
        "\n",
        "    def __init__(self, model, model_dir):\n",
        "        self.model = model\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        ckpt = tf.train.Checkpoint(\n",
        "            step=tf.Variable(0, dtype=tf.int64),\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            model=self.model)\n",
        "        ckpt_manager = tf.train.CheckpointManager(\n",
        "            ckpt, str(self.model_dir), max_to_keep=2)\n",
        "\n",
        "        if ckpt_manager.latest_checkpoint:\n",
        "            ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
        "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
        "\n",
        "        risk_scores = []\n",
        "        for batch_image, batch_num in dataset:\n",
        "            pred = self.model([batch_image, batch_num], training=False)\n",
        "            risk_scores.append(pred.numpy())\n",
        "\n",
        "        return np.row_stack(risk_scores)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogo8GhHxP_vs"
      },
      "source": [
        "**On Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lu4ICJ5ngNx"
      },
      "source": [
        "train_pred_fn_1 = tf.data.Dataset.from_tensor_slices(x_train[..., np.newaxis]).batch(20)\n",
        "train_pred_fn_2 = tf.data.Dataset.from_tensor_slices(clinical_train).batch(20)\n",
        "train_pred_fn = tf.data.Dataset.zip((train_pred_fn_1,train_pred_fn_2))\n",
        "\n",
        "predictor = Predictor(model, trainer.model_dir)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQWCeadv5qxd"
      },
      "source": [
        "#Predicted risk score of train data\r\n",
        "train_predictions = predictor.predict(train_pred_fn)\r\n",
        "\r\n",
        "risk_score_train=pd.DataFrame(train_predictions)\r\n",
        "risk_score_train['time_train']=time_train\r\n",
        "risk_score_train['event_train']=event_train\r\n",
        "risk_score_train=risk_score_train.rename(columns={0:'risk_score'})\r\n",
        "risk_score_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xtd_begQu5y"
      },
      "source": [
        "**On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQEgwcD_Qz-N"
      },
      "source": [
        "#Predicted risk score of validation data\r\n",
        "sample_pred_ds_1 = tf.data.Dataset.from_tensor_slices(x_test[..., np.newaxis]).batch(20)\r\n",
        "sample_pred_ds_2 = tf.data.Dataset.from_tensor_slices(clinical_test).batch(20)\r\n",
        "sample_pred_ds = tf.data.Dataset.zip((sample_pred_ds_1,sample_pred_ds_2))\r\n",
        "\r\n",
        "sample_predictions = predictor.predict(sample_pred_ds)\r\n",
        "\r\n",
        "risk_score_val=pd.DataFrame(sample_predictions)\r\n",
        "risk_score_val['time_test']=time_test\r\n",
        "risk_score_val['event_test']=event_test\r\n",
        "risk_score_val=risk_score_val.rename(columns={0:'risk_score'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAViqK4TQ-aT"
      },
      "source": [
        "### **Cox Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6eqA74VUG4w"
      },
      "source": [
        "**On Train Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBB_7GvoRG6c"
      },
      "source": [
        "This is a univariate Cox model with the predicted risk score estimated from the 3D CT scans and the clinical data as explanatory variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsiWmpAZ5zgT"
      },
      "source": [
        "breslow = cph.fit(risk_score_train, duration_col=\"time_train\", event_col=\"event_train\")\n",
        "breslow.print_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iOQMPs8RbI6"
      },
      "source": [
        "**On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD3mY3Ck6Ae5"
      },
      "source": [
        "# Expected Lifetime\n",
        "risk_score_val['predictions']=breslow.predict_expectation(risk_score_val)\n",
        "\n",
        "#Concordance Index\n",
        "print(f'Concordance index (lifelines): {concordance_index(risk_score_val.time_test, risk_score_val.predictions, risk_score_val.event_test)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmIwnvRKCkXS"
      },
      "source": [
        "### **Predictions of Expected Lifetime on Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPYItwwWCnUX"
      },
      "source": [
        "# Loading Data\n",
        "test_folder = pathlib.Path(\"/content/drive/My Drive/x_test/images\")\n",
        "all_image_paths_test = [str(img_path) for img_path in list(test_folder.glob(\"*\"))]\n",
        "all_image_paths_test = sorted(all_image_paths_test)\n",
        "normal_scans_test = np.array([process_scan(path) for path in all_image_paths_test])\n",
        "\n",
        "clinical_test=pd.read_csv(\"/content/drive/My Drive/x_test/features/clinical_data_test.csv\", index_col='PatientID')\n",
        "\n",
        "#Preprocessing\n",
        "clinical_test['Nstage']=clinical_test.Nstage.apply(lambda x: 3 if x==4 else x)\n",
        "clinical_test['Tstage']=clinical_test.Tstage.apply(lambda x: 4 if x==5 else x)\n",
        "clinical_test['Histology_cat']=clinical_test.Histology.apply(lambda x: 0 if x in ('Adenocarcinoma','adenocarcinoma') \n",
        "                                                               else 1 if x=='large cell' \n",
        "                                                               else 2 if x in('squamous cell carcinoma', 'Squamous cell carcinoma') \n",
        "                                                               else 3)\n",
        "categories = ['Nstage','Tstage','Histology_cat']\n",
        "clinical_test = pd.get_dummies(clinical_test, columns=categories, drop_first=True)\n",
        "clinical_test=clinical_test.drop(columns=['Histology','Mstage'])\n",
        "clinical_test=clinical_test.sort_index()\n",
        "clinical_test.SourceDataset=clinical_test.SourceDataset.apply(lambda x: 0 if x=='l1' else 1)\n",
        "\n",
        "clinical_test_= imp_mean.transform(clinical_test)\n",
        "clinical_test=pd.DataFrame(clinical_test_, index=clinical_test.index, columns=clinical_test.columns)\n",
        "clinical = clinical_test.to_numpy(dtype=\"float32\")\n",
        "\n",
        "#Predicted risk score of test data\n",
        "sample_pred_ds_1 = tf.data.Dataset.from_tensor_slices(normal_scans_test[..., np.newaxis]).batch(20)\n",
        "sample_pred_ds_2 = tf.data.Dataset.from_tensor_slices(clinical).batch(20)\n",
        "sample_pred_ds = tf.data.Dataset.zip((sample_pred_ds_1,sample_pred_ds_2))\n",
        "\n",
        "sample_predictions = predictor.predict(sample_pred_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMBEMA_gdK1B"
      },
      "source": [
        "# Expected Lifetime\r\n",
        "risk_score_test=pd.DataFrame(data=sample_predictions)\r\n",
        "risk_score_test=risk_score_test.rename(columns={0:'risk_score'})\r\n",
        "\r\n",
        "pred=breslow.predict_expectation(risk_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZrb9ZRUHHCf"
      },
      "source": [
        "output=pd.DataFrame(data=pred)\n",
        "output=output.rename(columns={0:'SurvivalTime'})\n",
        "output['Event']='nan'\n",
        "output.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}